#notes: lower temepratre or tell ai to make samll changes, keep feedback small
#better prompts: reinforcement learning?
#have memory: let LLM see what does and doesnt work throughout iterations
#consider using a dedicated prompt manager
from openai import OpenAI
#generate multiple prompts, and choose the most effective(gets highest rating) to continue
def prompt_generator(current_prompt, feedback):
    client = OpenAI()
    prompt1 = f"You are given the folowing prompt: {current_prompt}, and feedback on the mistakes in the response generated by it: {feedback}. Create another prompt, improving on the current prompt, that will address the points in the feedback. Use all your expertise as a prompt engineer with knowledge of best practice for prompt engineering. Return the new prompt and the new prompt only, or you will be fired. Keep in mind. "

    
    prompt_gen = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": "I want you to assume the role of an expert prompt engineer. You are a flawless engineer who writes prompts that generate perfectly accurate responses. You write production quality prompts that are clean, clear, and follows the newest and most cutting edge best practices of prompt engineering. These practices include roleplay, specific details, examples, specifying response type, "},
            {"role": "user", "content": prompt1},
        ]
    )
    return prompt_gen.choices[0].message.content